---
Title | 吴恩达机器学习课程 4 多变量线性回归
-- | --
Create Date | `2019-07-18T14:01:51Z`
Update Date | `2022-01-11T08:49:07Z`
Edit link | [here](https://github.com/junxnone/csc/issues/15)

---

# 1.多变量线性回归的假设
![image](https://user-images.githubusercontent.com/2216970/51480155-4bdf4200-1dcb-11e9-90e8-587f8214bbf6.png)  
即 ![image](https://user-images.githubusercontent.com/2216970/51480261-8c3ec000-1dcb-11e9-84e5-586a1c8d5326.png)

# 2.多变量线性回归的代价函数
![image](https://user-images.githubusercontent.com/2216970/51480337-c1e3a900-1dcb-11e9-8940-c0b533648efd.png)

# 3.多变量线性回归的批量梯度下降算法
![image](https://user-images.githubusercontent.com/2216970/51480387-e3449500-1dcb-11e9-86f7-cfeec3eb6cda.png)   

即：  
![image](https://user-images.githubusercontent.com/2216970/51480392-e8094900-1dcb-11e9-8718-d75a0d8c889d.png)  

求导数后得到：  
![image](https://user-images.githubusercontent.com/2216970/51480395-eb043980-1dcb-11e9-86ca-f10c2a94b05d.png)  

更新参数：  
![image](https://user-images.githubusercontent.com/2216970/51480624-7382da00-1dcc-11e9-97a3-a6e8a49c567c.png)  
![image](https://user-images.githubusercontent.com/2216970/51480631-767dca80-1dcc-11e9-9366-913b2647f318.png)  
![image](https://user-images.githubusercontent.com/2216970/51480636-78478e00-1dcc-11e9-8ea4-c626aa5cc169.png)  
……  
……  
# 4.特征缩放
> 面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

当两个特征有较大的数量级差异时，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。
解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。

# 5.学习率
- 如果学习率过小，则达到收敛所需的迭代次数会非常高；
- 如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

# 6.特征和多项式回归
线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如二次方模型或者三次方模型：  
![image](https://user-images.githubusercontent.com/2216970/51485551-5b658780-1dd9-11e9-8f6f-287364e4b342.png)  
> 多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

# 7.正规方程
对代价函数求导
> 对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。

# 8.梯度下降与正规方程的比较
梯度下降 | 正规方程
-- | --
需要选择学习率a | 不需要
需要多次迭代 | 一次运算得出
当特征数量n大时也能较好适用 | 需要计算![image](https://user-images.githubusercontent.com/2216970/51485975-6ff64f80-1dda-11e9-8b19-b095a7d069e0.png) 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为![image](https://user-images.githubusercontent.com/2216970/51485977-7258a980-1dda-11e9-9196-dbb954f2b792.png)，通常来说当n小于10000 时还是可以接受的
适用于各种类型的模型 | 只适用于线性模型，不适合逻辑回归模型等其他模型


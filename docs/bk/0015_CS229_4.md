-----

| Title     | CS229 4                                           |
| --------- | ------------------------------------------------- |
| Created @ | `2019-07-18T14:01:51Z`                            |
| Updated @ | `2024-01-29T16:17:30Z`                            |
| Labels    | `bug`                                             |
| Edit @    | [here](https://github.com/junxnone/csc/issues/15) |

-----

# 4 多变量线性回归

## 1.多变量线性回归的假设

![image](media/843ce82f672d56bc8d84ea3141d881bf1195e254.png)  
即 ![image](media/eca894bac421e0fac77f828a900be18d9c147d13.png)

## 2.多变量线性回归的代价函数

![image](media/c6251c130a6dd04ff46d552161fdea6bee2c7aa1.png)

## 3.多变量线性回归的批量梯度下降算法

![image](media/d71ccdeb22c67919bab838f429dba0f408889c48.png)

即：  
![image](media/d1105a3acb7699ca88cfd0b817259fcff7e05450.png)

求导数后得到：  
![image](media/4fe1479b13c90c3a5eb9a63de7bfb35a2af4bf63.png)

更新参数：  
![image](media/22046dce1286828411b1ecc4dfe7fab2d0cd43ad.png)  
![image](media/4a05dd2ebccf13b63342cf9f780d84fb74febcf2.png)  
![image](media/323950a7561c343779565d9d007ebb390bb81d69.png)  
……  
……

## 4.特征缩放

> 面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

当两个特征有较大的数量级差异时，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。
解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。

## 5.学习率

  - 如果学习率过小，则达到收敛所需的迭代次数会非常高；
  - 如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

## 6.特征和多项式回归

线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如二次方模型或者三次方模型：  
![image](media/7fdd25e687538db24a9200c5f1ca9c2d25e9d10d.png)

> 多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

## 7.正规方程

对代价函数求导

> 对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。

## 8.梯度下降与正规方程的比较

| 梯度下降           | 正规方程                                                                                                                                                                               |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 需要选择学习率a       | 不需要                                                                                                                                                                                |
| 需要多次迭代         | 一次运算得出                                                                                                                                                                             |
| 当特征数量n大时也能较好适用 | 需要计算![image](media/bf8451bd83d96b51dc5abd9996c5d5656d402fb2.png) 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为![image](media/10b952b4ed83b24f91926da0724312ec05969887.png)，通常来说当n小于10000 时还是可以接受的 |
| 适用于各种类型的模型     | 只适用于线性模型，不适合逻辑回归模型等其他模型                                                                                                                                                            |
